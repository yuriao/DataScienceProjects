{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuriao/DataScienceProjects/blob/main/%5BICME_NLP%5D_Notebook_2_Sentiment_extraction_with_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAatSr9BnLA9"
      },
      "source": [
        "# Practical use case: Sentiment extraction with BERT\n",
        "*Afshine Amidi, Shervine Amidi*\n",
        "\n",
        "*Introduction to Natural Language Processing, Stanford ICME Summer workshop 2023*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6pzNml0ugwF"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwa83KiUhML_"
      },
      "source": [
        "### Pretty printing in Colab\n",
        "\n",
        "First, let's set one detail: make Colab print arrays one element per line. To do so, we use the `pprint` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JABs6CNxhoW5"
      },
      "source": [
        "import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP4hyCOECnY2"
      },
      "source": [
        "### Packages installation\n",
        "\n",
        "Some of the most used packages are pre-installed along with default Colab runtimes. That means that we don't have to install all the packages at initialization, which is great!\n",
        "\n",
        "However when others are missing, it's really simple to make up for it! Just use the `!` symbol followed by the command line you would enter in a Terminal session to install the remaining packages.\n",
        "\n",
        "Here, we do this procedure for all HuggingFace-related modules needed for data and model preparation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgx_wC51fJ15"
      },
      "source": [
        "!pip install datasets transformers huggingface_hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QsrfXX-HKYa"
      },
      "source": [
        "### Imports\n",
        "\n",
        "For better readability, we gather all import statements at a same place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqjTH1fkCcOQ"
      },
      "source": [
        "import datasets\n",
        "import huggingface_hub\n",
        "import random\n",
        "import transformers\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHX7b2TcGEn_"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint.pprint([x.id for x in huggingface_hub.list_datasets()][:30])"
      ],
      "metadata": {
        "id": "rpbouy_vm4fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyDRGPCFul4L"
      },
      "source": [
        "### Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyLxdIlwq4ZJ"
      },
      "source": [
        "#### From the public"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUcEBvcog9UF"
      },
      "source": [
        "NUM_LABELS = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8nZEyVVGVSQ"
      },
      "source": [
        "train_dataset = datasets.load_dataset('imdb', split='train')\n",
        "test_dataset = datasets.load_dataset('imdb', split='test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPX4tVdtiBsJ"
      },
      "source": [
        "### Inspection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXiQ26vHHJpJ"
      },
      "source": [
        "def print_samples_from_dataset(dataset: datasets.Dataset,\n",
        "                               target_label: int = None,\n",
        "                               max_print_count: int = 1,\n",
        "                               shuffle: bool = True) -> None:\n",
        "  \"\"\"Prints samples from the dataset.\"\"\"\n",
        "  # Shuffle the dataset if the parameter is set to True.\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(random.randint(0, 10e6))\n",
        "\n",
        "  # Loop over dataset samples.\n",
        "  print_count = 0\n",
        "  for i, sample in enumerate(dataset):\n",
        "    # Ignore labels that do not correspond to the one\n",
        "    # we are looking for.\n",
        "    if sample['label'] != target_label:\n",
        "      continue\n",
        "\n",
        "    pprint.pprint(sample)\n",
        "    print_count += 1\n",
        "\n",
        "    # Stop condition.\n",
        "    if print_count == max_print_count:\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2enfvhQlj6is"
      },
      "source": [
        "print_samples_from_dataset(train_dataset, target_label=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJlDUrrYlvDo"
      },
      "source": [
        "print_samples_from_dataset(train_dataset, target_label=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIJBtyYUl2oK"
      },
      "source": [
        "### Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn64OgnVmzG2"
      },
      "source": [
        "def count_samples(dataset, target_label=None):\n",
        "  total_count = 0\n",
        "  for i, sample in enumerate(dataset):\n",
        "    if target_label is None or sample['label'] != target_label:\n",
        "      continue\n",
        "    total_count += 1\n",
        "  return total_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLSANx2lHoyn"
      },
      "source": [
        "print(f'Training set contains {len(train_dataset)} samples of which')\n",
        "print(f'  {count_samples(train_dataset, target_label=0)} are negative and')\n",
        "print(f'  {count_samples(train_dataset, target_label=1)} are positive.')\n",
        "print()\n",
        "print(f'Test set contains {len(test_dataset)} samples of which')\n",
        "print(f'  {count_samples(test_dataset, target_label=0)} are negative and')\n",
        "print(f'  {count_samples(test_dataset, target_label=1)} are positive.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQCyzqjFusmD"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU40ML2Hm0Ea"
      },
      "source": [
        "#### Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0jE-q6Om3GV"
      },
      "source": [
        "First, we start by loading a pre-trained WordPiece tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCNSiQtZ962K"
      },
      "source": [
        "tokenizer = transformers.BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48EPmoxsm8rH"
      },
      "source": [
        "#### Inspection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j90N7KjV_Wo"
      },
      "source": [
        "vocab_dict = tokenizer.get_vocab()\n",
        "sorted_vocab_list = sorted(vocab_dict.items(), key=lambda string_to_id: string_to_id[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP18Z69SfTq7"
      },
      "source": [
        "Unused tokens and special characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHqmzsU1e_JC"
      },
      "source": [
        "print(sorted_vocab_list[:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGqeuMsMfYfC"
      },
      "source": [
        "Subwords sorted by order of decreasing frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFy3-jP9fDHI"
      },
      "source": [
        "pprint.pprint(sorted_vocab_list[2000:2020])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEz7gZZanJB9"
      },
      "source": [
        "#### Fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxEo-284nKmw"
      },
      "source": [
        "Now that we saw that the loaded tokenizer looks reasonable, we can fit our dataset to produce the tokens that we will later feed to the model. Here, from HuggingFace's [docs](https://huggingface.co/transformers/preprocessing.html), we choose:\n",
        "- a padding parameter such that the input is padded up to the size expected by the model\n",
        "- to truncate the input, in case it exceeds the model's expected input size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHjA4n5l4XrK"
      },
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyF2kg776cng"
      },
      "source": [
        "This operations turns the tokenized version of the dataset into a `Dataset` object as well, which avoids loading all expected tokens in memory. Instead the tokenized versions of the data is saved on disk and ready to be loaded at training time.\n",
        "\n",
        "The `batched` boolean enables processing tokens by batch of samples (of default size 1000), which is much more efficient computation-wise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Em6oXEn3xkW"
      },
      "source": [
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD4fQXJI_kEp"
      },
      "source": [
        "tokenized_train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIEZG0YwDWXf"
      },
      "source": [
        "tokenizer.model_input_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnwuXcxZ-vDt"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "def get_tf_dataset(hf_dataset):\n",
        "  \"\"\"Transforms HuggingFace dataset into a TF dataset feedable to Keras.\"\"\"\n",
        "  # Objects returned by the HuggingFace Dataset object are now TensorFlow\n",
        "  # tensors.\n",
        "  hf_dataset = hf_dataset.with_format('tensorflow')\n",
        "\n",
        "  # Prepare input.\n",
        "  X = {col: hf_dataset[col] for col in tokenizer.model_input_names}\n",
        "  y = hf_dataset['label']\n",
        "\n",
        "  # Create TensorFlow dataset.\n",
        "  tf_dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "\n",
        "  # VERY important since IMDB data has grouped positive and negative reviews\n",
        "  # together: Shuffle the order which the samples are generated from the\n",
        "  # dataset.\n",
        "  tf_dataset = tf_dataset.shuffle(len(tf_dataset))\n",
        "\n",
        "  # Specify the batch size to be used in the model.\n",
        "  tf_dataset = tf_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "  return tf_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UNXpef889Qx"
      },
      "source": [
        "tf_tokenized_train_dataset = get_tf_dataset(tokenized_train_dataset)\n",
        "tf_tokenized_test_dataset = get_tf_dataset(tokenized_test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isXOUiiyGOO5"
      },
      "source": [
        "next(iter(tf_tokenized_train_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBLo2XMcwgpz"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0FT0vzmwjS7"
      },
      "source": [
        "### Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIhr1ixdrZvi"
      },
      "source": [
        "#### From a pre-trained checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFC45FxzwhTi"
      },
      "source": [
        "model = transformers.TFBertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased', num_labels=NUM_LABELS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF-2jlKwqseE"
      },
      "source": [
        "#### Input/output sanity check\n",
        "Let's do one forward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F3_bcXABeVU"
      },
      "source": [
        "inputs = tokenizer(\"So sad that this workshop is ending soon. Disgusted.\", return_tensors=\"tf\")\n",
        "inputs[\"labels\"] = tf.reshape(tf.constant(1),  # Label 1: positive sentiment.\n",
        "                              (-1, 1)  # Resizing to have a shape of (batch_size, label) == (1, 1)\n",
        "                              )\n",
        "outputs = model(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQsdyIDsDISN"
      },
      "source": [
        "print('Inputs')\n",
        "pprint.pprint(inputs)\n",
        "print()\n",
        "print('Outputs')\n",
        "pprint.pprint(outputs)\n",
        "print()\n",
        "print('Logits')\n",
        "pprint.pprint(outputs.logits)\n",
        "print()\n",
        "print('Loss')\n",
        "pprint.pprint(outputs.loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq5NLErcsRpP"
      },
      "source": [
        "Now let's check that the computed loss is consistent with the model output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2KC_NqRf-VT"
      },
      "source": [
        "# Output logits.\n",
        "print('Logits given by the model')\n",
        "print(outputs.logits)\n",
        "print()\n",
        "\n",
        "# Pass output logit to a softmax layer.\n",
        "# softmax_values = exp(logit_values) / sum(exp(logit_values))\n",
        "softmaxed_output = tf.nn.softmax(outputs.logits)\n",
        "print('After softmax')\n",
        "print(softmaxed_output)\n",
        "print()\n",
        "\n",
        "# Apply binary cross-entropy formula:\n",
        "# loss = - [ y log(p) + (1 - y) log(1 - p) ]\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)(inputs['labels'], softmaxed_output)\n",
        "print('After applying the binary cross-entropy formula')\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vct0L-6Iwmh1"
      },
      "source": [
        "#### Inspect the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4MmmCdkpV1F"
      },
      "source": [
        "The summary of the model can be found with the command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YAFgnPdytUr"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPo8JOGmpYbt"
      },
      "source": [
        "Let's inspect the parameters with which the BERT layer was trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am4NNin6pBJo"
      },
      "source": [
        "model.layers[0].get_config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFpCjzLbpc9q"
      },
      "source": [
        "HuggingFace's standard `TFBertForSequenceClassification` model adds us a dropout layer of parameter $p=0.1$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuDLi7pAjOzD"
      },
      "source": [
        "model.layers[1].get_config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkqOSxdgppJP"
      },
      "source": [
        "as well as a fully-connected layer of input size 768 and output size of `NUM_CLASSES`, i.e. 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4PbFqdZpLpj"
      },
      "source": [
        "model.layers[2].get_config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2JJ8E3twp-M"
      },
      "source": [
        "tf.keras.utils.plot_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXvf_odJwlvx"
      },
      "source": [
        "### Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g5p4GTec_aS"
      },
      "source": [
        "#### Freeze pre-trained weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqjkpcN8deyX"
      },
      "source": [
        "First, we inspect the layers that we have at hand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q63lkV4rwooD"
      },
      "source": [
        "for layer in model.layers:\n",
        "  print(type(layer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr_ntkRidjIs"
      },
      "source": [
        "Now, we freeze all layers, except for the last one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goX0cfzedoHD"
      },
      "source": [
        "for layer in model.layers:\n",
        "  # For all layers except the last one, tell Keras that the weights\n",
        "  # should remain frozen. Effectively, this operation will only apply\n",
        "  # to the main BERT layer 'TFBertMainLayer'.\n",
        "  if type(layer) != tf.keras.layers.Dense:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II_MWV6DfyQ9"
      },
      "source": [
        "Let's check that only the only trainable parameters are those of the last layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBfVJwuuf3Jg"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9uim_Lhic_m"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mcl9tPPPI2iI"
      },
      "source": [
        "LEARNING_RATE = 5e-5\n",
        "\n",
        "# Disable TensorFlow warnings that are unrelated to the\n",
        "# computations.\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7DghWrAige1"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVFlbnoRPUi9"
      },
      "source": [
        "model.fit(tf_tokenized_train_dataset, validation_data=tf_tokenized_test_dataset, epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3MLrvavF7V5"
      },
      "source": [
        "# The training process reaches ~95% train accuracy after training on all model\n",
        "# weights for ~1-2 epochs."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFsRudEhiefX"
      },
      "source": [
        "#### Predictions after training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Jg1PtGqH3k-"
      },
      "source": [
        "##### Load already fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMywZTwWjeXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5155fd90-c130-4576-a553-b4458c098fdb"
      },
      "source": [
        "!wget https://stanford.edu/~shervine/imdb_model_after_several_epochs.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-08 19:24:09--  https://stanford.edu/~shervine/imdb_model_after_several_epochs.zip\n",
            "Resolving stanford.edu (stanford.edu)... 171.67.215.200, 2607:f6d0:0:925a::ab43:d7c8\n",
            "Connecting to stanford.edu (stanford.edu)|171.67.215.200|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 405584029 (387M) [application/zip]\n",
            "Saving to: ‘imdb_model_after_several_epochs.zip’\n",
            "\n",
            "imdb_model_after_se 100%[===================>] 386.79M  2.60MB/s    in 2m 40s  \n",
            "\n",
            "2023-08-08 19:26:50 (2.41 MB/s) - ‘imdb_model_after_several_epochs.zip’ saved [405584029/405584029]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUn7c-NWjuEk"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "file_name = 'imdb_model_after_several_epochs.zip'\n",
        "\n",
        "with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
        "  zip_ref.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDhfiz5yH6cq"
      },
      "source": [
        "# We load the model by simply giving the file path:\n",
        "model = transformers.TFBertForSequenceClassification.from_pretrained(\"imdb_model_after_several_epochs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WpbU-2xHP0A"
      },
      "source": [
        "##### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-OeIAI4GHvu"
      },
      "source": [
        "# This package is handy to track for loops progressions.\n",
        "import tqdm\n",
        "\n",
        "def compute_accuracy(tf_tokenized_dataset):\n",
        "  \"\"\"Predicts model accuracy over a dataset.\"\"\"\n",
        "  # Initialize counters.\n",
        "  n_samples = 0\n",
        "  total_correct_pred = 0\n",
        "\n",
        "  # Iterate over entire dataset.\n",
        "  for sample in tqdm.tqdm(tf_tokenized_dataset):\n",
        "    # Compute model outputs. Please note that the output are logits and not\n",
        "    # probabilities, the latter being computed at the loss stage.\n",
        "    output_logits = model.predict(sample).logits\n",
        "\n",
        "    # Get predicted and true labels.\n",
        "    pred_labels = tf.argmax(output_logits, axis=1)\n",
        "    true_labels = sample[1]\n",
        "\n",
        "    # A prediction is accurate when the predicted label equals the true label.\n",
        "    correct_pred = tf.reduce_sum(tf.cast(pred_labels == true_labels, tf.float32)).numpy()\n",
        "\n",
        "    # Some bookkeeping.\n",
        "    total_correct_pred += correct_pred\n",
        "    n_samples += len(true_labels)\n",
        "\n",
        "  # Proportion of correct predictions.\n",
        "  return total_correct_pred / n_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bg5j5QUHHdu"
      },
      "source": [
        "# This is equivalent to just doing model.evaluate(tf_tokenized_test_dataset).\n",
        "# The function above gives some more details about what happens under the hood.\n",
        "compute_accuracy(tf_tokenized_test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltLL2y_YHUcU"
      },
      "source": [
        "##### Playground"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRieYnqIHWDu"
      },
      "source": [
        "SENTIMENTS = ['negative', 'positive']\n",
        "\n",
        "def generate_sample_and_prediction(tf_tokenized_dataset):\n",
        "  \"\"\"Generates a sample and its predictions.\"\"\"\n",
        "  sample = next(iter(tf_tokenized_dataset))\n",
        "  output_logits = model.predict(sample).logits\n",
        "  output_probabilities = tf.nn.softmax(output_logits)\n",
        "\n",
        "  tokenized_sentence = sample[0]['input_ids'][0]\n",
        "  true_label = sample[1][0].numpy()\n",
        "  pred_distribution = output_probabilities[0,:].numpy()\n",
        "\n",
        "  print('Tokenized sentence: ')\n",
        "  pprint.pprint(tokenizer.decode(tokenized_sentence))\n",
        "  print()\n",
        "  print(f'This review is labeled as {SENTIMENTS[true_label]}.')\n",
        "  print()\n",
        "  print('Model predicts:')\n",
        "  print(f'{100 * pred_distribution[0]:.2f} % negative')\n",
        "  print(f'{100 * pred_distribution[1]:.2f} % positive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQzkwPj3NVfZ"
      },
      "source": [
        "generate_sample_and_prediction(tf_tokenized_test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaU2pB7jxTvR"
      },
      "source": [
        "## References\n",
        "### Papers\n",
        "- Devlin et al, 2018. *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*. [arXiv:1810.04805](https://arxiv.org/pdf/1810.04805.pdf).\n",
        "\n",
        "### Datasets\n",
        "- Maas et al., 2011. *Large Movie Review Dataset*. [ai.stanford.edu/~amaas/data/sentiment/](https://ai.stanford.edu/~amaas/data/sentiment/). Unlicensed.\n",
        "\n",
        "### Posts\n",
        "- Amidi, 2018. *A detailed example of how to use data generators with Keras*. [stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly](https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly). Unlicensed.\n",
        "- Amidi, 2018. *A detailed example of how to generate your data in parallel with PyTorch*. [stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel](https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel). Unlicensed.\n",
        "- HuggingFace, 2023. *Fine-tune a pretrained model*. [huggingface.co/transformers/training](https://huggingface.co/transformers/training.html). [Apache License 2.0](https://github.com/huggingface/transformers/blob/master/LICENSE).\n",
        "- HuggingFace, 2023. *BERT*. [huggingface.co/transformers/model_doc/bert](https://huggingface.co/transformers/model_doc/bert.html). [Apache License 2.0](https://github.com/huggingface/transformers/blob/master/LICENSE).\n",
        "- HuggingFace, 2023. *Auto Classes*. [huggingface.co/transformers/model_doc/auto](https://huggingface.co/transformers/model_doc/auto.html). [Apache License 2.0](https://github.com/huggingface/transformers/blob/master/LICENSE)."
      ]
    }
  ]
}